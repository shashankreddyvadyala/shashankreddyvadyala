Slide 1: Title Slide
Title: Navigating the Advanced Analytics Workflow
Subtitle: Unleashing the Power of EFS & Bitbucket Integration
Imagery: Dynamic background image showcasing data analytics visualizations, icons of EFS and Bitbucket.
Quick Takeaway: Brief overview of the agenda – understanding the workflow from analysis initiation to data management and security protocols.
Slide 2: Analysis Initiation
Title: Kickstarting the Analytical Journey
Points:
The synergy of Bitbucket and EFS in real-time data analysis.
Auto-setup of repositories and folders ensuring seamless user experience.
Instant access for registered users, enhancing collaboration.
Imagery: Process flow diagram illustrating the steps from analysis initiation to user access.
Deep Dive: Highlight of user roles – Analysis Lead and Contributors, and their automatic integration into the system.
Slide 3: EFS Folder Structure & Naming
Title: A Glimpse into EFS Organization
Points:
Folder taxonomy based on Therapeutic Area, easing data categorization.
Strict adherence to GxP guidelines, ensuring regulatory compliance.
Unique identification via RWDEx generated Analysis numbers.
Imagery: Visual representation of folder structure with labels, demonstrating organization.
In Focus: Real-life examples of how therapeutic areas and analyses are systematically arranged.
Slide 4: RWE Collaborative Code Hub
Title: Centralizing Code Management
Points:
Introduction to "inprogress" - the developmental sanctuary for code.
The meticulous journey of code from development to Bitbucket’s master branch.
Library Management Committee’s role in code approval and promotion.
Imagery: Flow chart of code’s journey from development, validation, to approval.
Insights: Spotlight on the checks and balances ensuring code integrity.
Slide 5: Levels of Code Validation
Title: Grading the Code
Points:
Differentiation between Jobaid, Knowledgelib, and Macrolib – outlining their validation levels.
The progression of code maturity and reliability.
The benchmark parameters ensuring code readiness for different stages.
Imagery: A comparative table or graph highlighting validation levels and criteria.
Exploration: Case studies exemplifying the application and progression of codes across validation levels.
Slide 6: Bitbucket’s Role
Title: Mastering Repository Management
Points:
Bitbucket as the sanctuary for the latest and approved versions of code.
Access control protocols ensuring security and data integrity.
Real-time updates facilitating seamless collaboration among developers.
Imagery: Screenshots of Bitbucket interface showcasing repository and access management features.
Analysis: Walkthrough of a typical user experience in accessing and managing code via Bitbucket.
Slide 7: Development Process
Title: Crafting the Code
Points:
The adherence to predefined directory structures for consistency.
Code referencing protocols ensuring reproducibility and collaboration.
The sanctity of Bitbucket as the reservoir of the most updated code versions.
Imagery: Step-by-step visual flow of code development, referencing, and updates.
Scenario: A real-time demonstration or animation showing the code development and update process.
Slide 8: Modifications & Adjustments
Title: Evolving with Needs
Points:
The systematic approach to requesting folder name and access modifications.
Specifics on parameters that are flexible for changes.
Support ticket routing and processing for efficient handling of modification requests.
Imagery: A flow chart or process diagram showing the steps to make modifications.
Example: A sample support ticket, showcasing the information required and the expected processing timeline.
Slide 9: Security & Compliance
Title: Safeguarding Data & Code
Points:
Reiteration of the GxP compliance in all facets of the workflow.
Security protocols in place for robust data and code protection.
Access controls ensuring that data integrity is uncompromised.
Imagery: Icons or visuals representing security locks, shields, and compliance badges.
Spotlight: Brief on the regular audits, checks, and updates ensuring ongoing compliance and security.
Slide 10: Wrapping Up
Title: The Journey Ahead
Points:
Recap of key insights on the integration, management, and security protocols.
Future trends – expected enhancements and upgrades to the EFS and Bitbucket integration.
Open invitation for feedback and collaborative improvement initiatives.
Imagery: A recap infographic or visual summary of key points.
Call to Action: Encouraging audience engagement, questions, and collaborative dialogues for continuous improvement.
Slide 11: Questions & Interaction
Title: Your Insights Matter
Points:
A dedicated slot for audience questions, insights, and discussions.
Encouraging an interactive session to explore real-time scenarios and solutions.
Imagery: Engaging graphic inviting questions and interactions.
Engagement Boost: Polls or interactive tools to facilitate live audience participation and feedback.
Each slide should be designed with visual elements that are consistent throughout the presentation, making it visually appealing and easy to follow. Use bullet points, visuals, and real-life examples to make each slide informative yet concise. Adapt and personalize as per the specific audience and context.





1.1.1 adhoc/
Under each analysis there will be several standard folders. The first folder is the adhoc folder. This is used for any adhoc/code/input/outputs that don’t need to be version controlled. Users are free to create subfolders under the adhoc folder as they see fit
The adhoc folder is to be used only when rest of the subfolder don't meet requirements. If the code in adhoc folder needs to be moved to production, it should be placed in teh code/src folder
1.1.2 code/ 
Under each analysis, the second folder is code. There are several characteristics to be noted here:

Code folder is language/tool agnostic. Code written in any language should be placed here

Code folder mirrors a repository in Bitbucket (for detailed information see  SAS, JH or R reproducibility WFs)

All code will be version controlled upon commiting into Bitbucket

Access to the code repository for any additional user (not marked as Contributors in Analysis Wizard) in bitbucket will be granted by the Support Team upon request

Only the Support Team can delete repositories from Bitbucket

This folder on EFS shoud always reflect the latest production version of the code from BitBucket. There is a triangular relationship in between the Local/home workspace, BitBucket, and EFS study folder:
The local workspace, aka. ~home folder, is used for everyday development
The BitBucket is to be used as target remote repository for storing the approved code, and represents the single source of truth
The EFS study folder is to be an access restricted folder where the finalized version of code is stored for reproducibility and archival purposes, and should mirror the latest BitBucket version
 Detailed Drill down
1.1.2.1 conf/

Under each code folder, the first folder is the configuration folder. This will contain any configuration or property files that are specific to the code
1.1.2.2 lib/
Under each code folder, the second folder is the library folder. This will contain any custom macros that are relevant to the analysis
1.1.2.3 src/
Under each code folder, the statistical programming/analysis should occur in the source folder. Users are free to create subfolders under source as they see fit
Subfolders such as modeling, preprocessing, preparation, extraction, transformation etc. can be created
1.1.2.4 study_files/
Any documentation/ study files/SOPs for the analysis should be placed here (eg. protocol, programming specifications, table mockups, icd/ndc/cpt code lists, etcl). Since the 1.1.2 code folder is a repository in bitbucket, the study files will be version controlled once checked into bitbucket. Users are free to create subfolders under study files as they see fitt
1.1.2.5 utility/
Under each code folder, the last folder is the utilities folder. This will contain any common procedures that are relevant to the analysis
1.1.3 input/
The third folder under each analysis is the input folder. All inputs files that are required for the analysis should be placed here
Users are free to create additional folders under the input folder aside from the standardized structure defined
 Detailed Drill down
1.1.3.1 data_raw/
Under each input folder, the first folder is the data raw folder. All preliminary/raw input datasets should be placed here
Datasets that are extracted from databases should be placed here
1.1.3.2 data_processed/
Under each input folder, the second folder is the data processed folder. All intermediary input datasets should be placed here.
Datasets that are outputs from code and are inputs to successive code should be placed here
1.1.3.3 ref/
Under each input folder, the third folder is the reference folder. Any lookup files, data dictionaries, ICD9/10/NDC codes or other reference material can be placed in this folder. Note: The source/raw version of icd/ndc/cpt codes is considered part of the analysis specifications and needs to be saved in the code repository/bitbucket so it is stored in /study_files often in excel format. The converted SAS/R version to be used in programs is stored here in /ref
1.1.4 output/
The fourth folder under each analysis is the output folder. All output files that are created for the analysis should be placed here.
Users are free to create additional folders under the output folder aside from the standardized structure defined
 Detailed Drill down
1.1.4.1 out_data/
Under each output folder, the first folder is the out data folder. All proprietary datasets such as .sas datasets that are generated from the code should be placed here
1.1.4.2 out_graphics/
Under each output folder, the second folder is the out graphics folder. All graphics or visualizations that are generated from the code should be placed here
1.1.4.3 out_logs/
Under each output folder, the third folder is the out logs folder. All logs that are generated from executing the code should be placed in this folder
1.1.4.4 out_lst/
Under each output folder, the fourth folder is the out lst folder. All data list files that are generated from the code should be placed here
1.1.4.5 out_tables/
Under each output folder, the last folder is the out tables folder. All exportable datasets (such as excel, csv, pdf files) that are generated from the analysis should be placed in this folder




