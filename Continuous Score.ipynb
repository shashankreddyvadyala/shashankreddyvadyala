{"cells":[{"cell_type":"code","source":["# Databricks notebook source\n# MAGIC %run /Users/eh163e@att.com/utl/set_service_principal\n\n# COMMAND ----------\n\nimport pandas as pd\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom datetime import datetime\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC # Read in DataWeb Whitelist and Blocklist\n\n# COMMAND ----------\n\ndate1 = '20220128'\ndate2 = str(datetime.now()).replace(\"-\",\"\")\nmydates = pd.date_range(date1, date2).tolist()\n\n# COMMAND ----------\n\nrun_date = str(mydates[0].date())\nroot = \"abfss://robofeed@famlisandbox.dfs.core.windows.net/robo_rpts/\"\nmobility_block_path = root +f\"research.{run_date}.MobilityBLOCK\"\nwhitelist_path = root +f\"research.{run_date}.Whitelist\"\nmobility_flagged_path1 = root +f\"research.{run_date}.RobocallMobility\"\nmobility_flagged_path2 = root +f\"research.{run_date}.RobocallMobility-8YY\"\nmobility_unblock_path = root +f\"research.{run_date}.MobilityUNBLOCK\"\nmobility_block_activity_path = root +f\"research.{run_date}.MobilityBlockActivity\"\nvm_pilot_path = 'abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/eh163e/data/robocalling/VM_ID_Pilot_Numbersd.csv'\n\ncnam_path =root+\"cnamcache.2022-02-21\"\n# cnam = (spark\n#   .read\n#   .format(\"CSV\")\n#   .option(\"header\",True)\n#   .option(\"delimiter\",\"|\")\n#   .load(cnam_path))\n\n# COMMAND ----------\n\nwhitelist = (spark\n  .read\n  .format(\"CSV\")\n  .option(\"header\",True)\n  .option(\"delimiter\",\"|\")\n  .load(whitelist_path)\n  .toPandas())\nwhitelist_total_lines = whitelist.iloc[0][2]\nwhitelist.columns = [\"Number\",\"cnam\",\"date\",\"_c3\"]\nwhitelist = whitelist[1:]\n\n# COMMAND ----------\n\n#whitelist = whitelist[1:]\nwhitelist.date = pd.to_datetime(whitelist.date,errors='coerce')\nwhitelist.date.max()\n\n# COMMAND ----------\n\nwhitelist\n\n\n# COMMAND ----------\n\nmobility_unblock[\"Date\"] = pd.to_datetime(mobility_unblock[\"Date\"])\nmobility_unblock[\"date\"] = mobility_unblock[\"Date\"].astype(str)\nmobility_unblock[\"feed\"]= \"unblock\"\nmobility_unblock\n\n# COMMAND ----------\n\nmobility_block[\"date\"]= run_date\nmobility_block[\"feed\"]= \"block\"\n\nmobility_block.head()\n\n# COMMAND ----------\n\nall_dfs = []\n\nfor date in mydates[0:2]:\n  run_date = str(date.date())\n  root = \"abfss://robofeed@famlisandbox.dfs.core.windows.net/robo_rpts/\"\n  mobility_block_path = root +f\"research.{run_date}.MobilityBLOCK\"\n  whitelist_path = root +f\"research.{run_date}.Whitelist\"\n  mobility_flagged_path1 = root +f\"research.{run_date}.RobocallMobility\"\n  mobility_flagged_path2 = root +f\"research.{run_date}.RobocallMobility-8YY\"\n  mobility_unblock_path = root +f\"research.{run_date}.MobilityUNBLOCK\"\n  #mobility_block_activity_path = root +f\"research.{run_date}.MobilityBlockActivity\"\n  vm_pilot_path = 'abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/eh163e/data/robocalling/VM_ID_Pilot_Numbersd.csv'\n\n  mobility_block = (spark\n    .read\n    .format(\"CSV\")\n    .option(\"header\",False)\n    .option(\"delimiter\",\"|\")\n    .load(mobility_block_path)\n    .toPandas())\n  mobility_block_totals = list(mobility_block.iloc[0])\n  mobility_block.columns = list(mobility_block.iloc[1])\n  mobility_block = mobility_block[2:]\n  mobility_block[\"run_date\"]= run_date # take max\n  mobility_block[\"date\"]= run_date # take min\n  mobility_block[\"feed\"]= \"block\"\n  all_dfs.append(mobility_block[['Number', 'date','feed',\"run_date\"]])\n\n#   mobility_block_activity = (spark\n#     .read\n#     .format(\"CSV\")\n#     .option(\"header\",False)\n#     .option(\"delimiter\",\"|\")\n#     .load(mobility_block_activity_path)\n#     .toPandas())\n#   mobility_block_activity_totals = list(mobility_block_activity.iloc[0])\n#   mobility_block_activity.columns = list(mobility_block_activity.iloc[1])\n#   mobility_block_activity = mobility_block_activity[2:]\n\n  whitelist = (spark\n    .read\n    .format(\"CSV\")\n    .option(\"header\",True)\n    .option(\"delimiter\",\"|\")\n    .load(whitelist_path)\n    .toPandas())\n  whitelist_total_lines = whitelist.iloc[0][2]\n  whitelist.columns = [\"Number\",\"cnam\",\"date\",\"_c3\"]\n  whitelist = whitelist[1:]\n  whitelist[\"run_date\"] = run_date\n  whitelist[\"feed\"]= \"whitelist\"\n  all_dfs.append(whitelist[['Number', 'date','feed',\"run_date\"]])\n\n  mobility_unblock = (spark\n    .read\n    .format(\"CSV\")\n    .option(\"header\",True)\n    .option(\"delimiter\",\"|\")\n    .load(mobility_unblock_path)\n    .toPandas())\n  mobility_unblock[\"Date\"] = pd.to_datetime(mobility_unblock[\"Date\"])\n  mobility_unblock[\"run_date\"] = run_date\n  mobility_unblock[\"date\"] = mobility_unblock[\"Date\"].astype(str)\n  mobility_unblock[\"feed\"]= \"unblock\"\n  all_dfs.append(mobility_unblock[['Number', 'date','feed',\"run_date\"]])\n\n  mobility_flagged1 = (spark\n    .read\n    .format(\"CSV\")\n    .option(\"header\",False)\n    .option(\"delimiter\",\"|\")\n    .load(mobility_flagged_path1)\n    .toPandas())\n  mobility_flagged1_totals = list(mobility_flagged1.iloc[0])\n  mobility_flagged1.columns = list(mobility_flagged1.iloc[1])\n  mobility_flagged1 = mobility_flagged1[2:]\n  mobility_flagged1.columns =['Number', 'num_of_OPCs', 'LNP_Lookup', 'num_of_Calls','DNC']\n  mobility_flagged1[\"run_date\"]= run_date\n  mobility_flagged1[\"date\"]= run_date\n  mobility_flagged1[\"feed\"]= \"flagged_mobility\"\n  all_dfs.append(mobility_flagged1[['Number', 'date','feed',\"run_date\"]])\n\n  mobility_flagged2 = (spark\n    .read\n    .format(\"CSV\")\n    .option(\"header\",False)\n    .option(\"delimiter\",\"|\")\n    .load(mobility_flagged_path2)\n    .select(\"_c0\",\"_c1\",\"_c3\",\"_c4\")\n    .toPandas())\n  mobility_flagged2_totals = list(mobility_flagged2.iloc[0])\n  mobility_flagged2.columns = list(mobility_flagged2.iloc[1])\n  mobility_flagged2 = mobility_flagged2[2:]\n  mobility_flagged2.columns =['Number', 'num_of_OPCs','num_of_Calls','DNC']\n  mobility_flagged2[\"run_date\"]= run_date\n  mobility_flagged2[\"date\"]= run_date\n  mobility_flagged2[\"feed\"]= \"flagged_mobility8YY\"\n  all_dfs.append(mobility_flagged2[['Number', 'date','feed',\"run_date\"]])\n\n  vm_pilot = (spark\n    .read\n    .format(\"CSV\")\n    .option(\"header\",True)\n    .option(\"delimiter\",\",\")\n    .load(vm_pilot_path)\n    .select(\"DEPOSIT_NUMBER\")  \n    .withColumnRenamed(\"DEPOSIT_NUMBER\",\"ctn\")\n    .toPandas())\n\n# COMMAND ----------\n\nfull_df = pd.concat(all_dfs)\nfull_df.date = pd.to_datetime(full_df.date,errors='coerce')\nfull_df.run_date = pd.to_datetime(full_df.run_date,errors='coerce')\n\n# COMMAND ----------\n\nfull_df_agg = full_df.groupby([\"Number\",\"feed\"]).agg(['min', 'max'])\nfull_df_agg\n\n# COMMAND ----------\n\nfull_df_agg.reset_index()\n\n# COMMAND ----------\n\n\n\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\nhistorical_robo_labels_path = \"abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/eh163e/data/robocalling/historical_robo_features_with_labels3\"\n\nfiles = dbutils.fs.ls(\"abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/call_graph/source/daily_refresh/\")\nall_dates = [file[1][:-1] for file in files if len(file[1]) > 7]\n\n# COMMAND ----------\n\n# add check condition on historical trusted edges date to see if already updated\ntry:\n  historical_robocalling = (spark\n     .read\n     .format('delta')\n     .load(historical_robo_labels_path))\n  historical_dates = historical_robocalling.select(\"Date\").distinct().collect()\nexcept Exception as e:\n    #print(e)\n    historical_dates = []\n\n#get dates in historical robocall labels\nif len(historical_dates) > 0:\n  historical_dates1 = [row['Date'] for row in historical_dates]\n  #subset dates to get dates not in historical table\n  dates_notin = [dt for dt in all_dates if (dt not in historical_dates1) & (dt[0]==\"2\")]\n  print(\"Dates data not present\")\n  print(dates_notin)\n\n# COMMAND ----------\n\n# iterate through the files and append them to an aggregated delta table for all robocall \nif len(dates_notin) > 0:\n  for date in dates_notin:\n    print(\"Proccessing Date:\",date)\n    if int(date) < 20220128:\n      # if date is before jan 28 when we don't have dataweb\n      try:\n        df = (spark\n              .read\n              .format(\"csv\")\n              .option(\"header\",True)\n              .load(f\"abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/call_graph/source/daily_refresh/{date}/features_extracted/\")\n            .withColumn(\"outgoing_call_count\", col(\"outgoing_call_count\").cast(IntegerType()))\n            .withColumn(\"incoming_call_count\", col(\"incoming_call_count\").cast(IntegerType()))\n            .withColumn(\"incoming_to_outgoing_ratio\", col(\"incoming_to_outgoing_ratio\").cast(DoubleType()))\n            .withColumn(\"outgoing_untrusted_call_count\", col(\"outgoing_untrusted_call_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_call_count\", col(\"outgoing_trusted_call_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_call_ratio\", col(\"outgoing_trusted_call_ratio\").cast(DoubleType()))\n            .withColumn(\"call_volume\", col(\"call_volume\").cast(IntegerType()))\n            .withColumn(\"otc_in_ratio\", col(\"otc_in_ratio\").cast(DoubleType()))\n            .withColumn(\"unique_ctn_called\", col(\"unique_ctn_called\").cast(IntegerType()))\n            .withColumn(\"mean_degree_one_outgoing_calls\", col(\"mean_degree_one_outgoing_calls\").cast(DoubleType()))\n            .withColumn(\"trusted_edge_count\", col(\"trusted_edge_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_text_count\", col(\"outgoing_text_count\").cast(IntegerType()))\n            .withColumn(\"incoming_text_count\", col(\"incoming_text_count\").cast(IntegerType()))\n            .withColumn(\"incoming_to_outgoing_ratio_text\", col(\"incoming_to_outgoing_ratio_text\").cast(DoubleType()))\n            .withColumn(\"outgoing_untrusted_text_count\", col(\"outgoing_untrusted_text_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_text_count\", col(\"outgoing_trusted_text_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_text_ratio\", col(\"outgoing_trusted_text_ratio\").cast(DoubleType()))\n            .withColumn(\"unique_ctn_texted\", col(\"unique_ctn_texted\").cast(IntegerType()))\n            .withColumn(\"mean_degree_one_outgoing_texts\", col(\"mean_degree_one_outgoing_texts\").cast(DoubleType()))\n            .withColumn(\"volume_cat\", col(\"volume_cat\").cast(IntegerType()))\n            .withColumn(\"repeated_calls\", col(\"repeated_calls\").cast(DoubleType()))\n            .withColumn(\"repeated_texts\", col(\"repeated_texts\").cast(DoubleType()))\n            .withColumn(\"text_volume\", col(\"text_volume\").cast(IntegerType()))\n            .withColumn(\"ott_in_ratio\", col(\"ott_in_ratio\").cast(DoubleType()))\n            .withColumn(\"risk_cat\", col(\"risk_cat\").cast(IntegerType()))\n            .withColumn(\"Date\", date_format(to_date(col(\"Date\")),\"yyyyMMdd\"))\n            ).fillna(0).fillna(0.0).fillna(False)\n        df.write.option(\"header\",True).partitionBy(\"Date\").option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").save(historical_robo_labels_path)\n      except Exception as e:\n        print(e)\n    else:\n      run_date = str(datetime.strptime(str(date), '%Y%m%d').date())\n      root = \"abfss://robofeed@famlisandbox.dfs.core.windows.net/robo_rpts/\"\n      mobility_block_path = root +f\"research.{run_date}.MobilityBLOCK\"\n      whitelist_path = root +f\"research.{run_date}.Whitelist\"\n      mobility_flagged_path1 = root +f\"research.{run_date}.RobocallMobility\"\n      mobility_flagged_path2 = root +f\"research.{run_date}.RobocallMobility-8YY\"\n      mobility_unblock_path = root +f\"research.{run_date}.MobilityUNBLOCK\"\n      mobility_block_activity_path = root +f\"research.{run_date}.MobilityBlockActivity\"\n      vm_pilot_path = 'abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/eh163e/data/robocalling/VM_ID_Pilot_Numbersd.csv'\n\n      mobility_block = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",False)\n        .option(\"delimiter\",\"|\")\n        .load(mobility_block_path)\n        .toPandas())\n      mobility_block_totals = list(mobility_block.iloc[0])\n      mobility_block.columns = list(mobility_block.iloc[1])\n      mobility_block = mobility_block[2:]\n\n      mobility_block_activity = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",False)\n        .option(\"delimiter\",\"|\")\n        .load(mobility_block_activity_path)\n        .toPandas())\n      mobility_block_activity_totals = list(mobility_block_activity.iloc[0])\n      mobility_block_activity.columns = list(mobility_block_activity.iloc[1])\n      mobility_block_activity = mobility_block_activity[2:]\n\n      mobility_unblock = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",True)\n        .option(\"delimiter\",\"|\")\n        .load(mobility_unblock_path)\n        .toPandas())\n\n      whitelist = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",True)\n        .option(\"delimiter\",\"|\")\n        .load(whitelist_path)\n        .toPandas())\n      whitelist_total_lines = whitelist.iloc[0][2]\n      whitelist.columns = [\"Number\",\"cnam\",\"date\",\"_c3\"]\n      whitelist = whitelist[1:]\n\n      mobility_unblock = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",True)\n        .option(\"delimiter\",\"|\")\n        .load(mobility_unblock_path)\n        .toPandas())\n\n      mobility_flagged1 = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",False)\n        .option(\"delimiter\",\"|\")\n        .load(mobility_flagged_path1)\n        .toPandas())\n      mobility_flagged1_totals = list(mobility_flagged1.iloc[0])\n      mobility_flagged1.columns = list(mobility_flagged1.iloc[1])\n      mobility_flagged1 = mobility_flagged1[2:]\n      mobility_flagged1.columns =['Number', 'num_of_OPCs', 'LNP_Lookup', 'num_of_Calls','DNC']\n\n      mobility_flagged2 = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",False)\n        .option(\"delimiter\",\"|\")\n        .load(mobility_flagged_path2)\n        .select(\"_c0\",\"_c1\",\"_c3\",\"_c4\")\n        .toPandas())\n      mobility_flagged2_totals = list(mobility_flagged2.iloc[0])\n      mobility_flagged2.columns = list(mobility_flagged2.iloc[1])\n      mobility_flagged2 = mobility_flagged2[2:]\n      mobility_flagged2.columns =['Number', 'num_of_OPCs','num_of_Calls','DNC']\n\n      vm_pilot = (spark\n        .read\n        .format(\"CSV\")\n        .option(\"header\",True)\n        .option(\"delimiter\",\",\")\n        .load(vm_pilot_path)\n        .select(\"DEPOSIT_NUMBER\")  \n        .withColumnRenamed(\"DEPOSIT_NUMBER\",\"ctn\")\n        .toPandas())\n\n      #combine all numbers to be filtered out, blocked, flagged, or whitelist\n      filter_out = [\"1\"+ctn for ctn in list(set(whitelist.Number)\n                                            .union(set(mobility_block.Number))\n                                            .union(set(vm_pilot.ctn))\n                                            .union(set(mobility_flagged1.Number))\n                                            .union(set(mobility_flagged2.Number)))]\n\n      try:\n        df = (spark.read.format(\"csv\").option(\"header\",True).load(f\"abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/call_graph/source/daily_refresh/{date}/features_extracted/\")\n        .withColumn(\"outgoing_call_count\", col(\"outgoing_call_count\").cast(IntegerType()))\n            .withColumn(\"incoming_call_count\", col(\"incoming_call_count\").cast(IntegerType()))\n            .withColumn(\"incoming_to_outgoing_ratio\", col(\"incoming_to_outgoing_ratio\").cast(DoubleType()))\n            .withColumn(\"outgoing_untrusted_call_count\", col(\"outgoing_untrusted_call_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_call_count\", col(\"outgoing_trusted_call_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_call_ratio\", col(\"outgoing_trusted_call_ratio\").cast(DoubleType()))\n            .withColumn(\"call_volume\", col(\"call_volume\").cast(IntegerType()))\n            .withColumn(\"otc_in_ratio\", col(\"otc_in_ratio\").cast(DoubleType()))\n            .withColumn(\"unique_ctn_called\", col(\"unique_ctn_called\").cast(IntegerType()))\n            .withColumn(\"mean_degree_one_outgoing_calls\", col(\"mean_degree_one_outgoing_calls\").cast(DoubleType()))\n            .withColumn(\"trusted_edge_count\", col(\"trusted_edge_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_text_count\", col(\"outgoing_text_count\").cast(IntegerType()))\n            .withColumn(\"incoming_text_count\", col(\"incoming_text_count\").cast(IntegerType()))\n            .withColumn(\"incoming_to_outgoing_ratio_text\", col(\"incoming_to_outgoing_ratio_text\").cast(DoubleType()))\n            .withColumn(\"outgoing_untrusted_text_count\", col(\"outgoing_untrusted_text_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_text_count\", col(\"outgoing_trusted_text_count\").cast(IntegerType()))\n            .withColumn(\"outgoing_trusted_text_ratio\", col(\"outgoing_trusted_text_ratio\").cast(DoubleType()))\n            .withColumn(\"unique_ctn_texted\", col(\"unique_ctn_texted\").cast(IntegerType()))\n            .withColumn(\"mean_degree_one_outgoing_texts\", col(\"mean_degree_one_outgoing_texts\").cast(DoubleType()))\n            .withColumn(\"volume_cat\", col(\"volume_cat\").cast(IntegerType()))\n            .withColumn(\"repeated_calls\", col(\"repeated_calls\").cast(DoubleType()))\n            .withColumn(\"repeated_texts\", col(\"repeated_texts\").cast(DoubleType()))\n            .withColumn(\"text_volume\", col(\"text_volume\").cast(IntegerType()))\n            .withColumn(\"ott_in_ratio\", col(\"ott_in_ratio\").cast(DoubleType()))\n            .withColumn(\"risk_cat\", col(\"risk_cat\").cast(IntegerType()))\n            .withColumn(\"Date\", date_format(to_date(col(\"Date\")),\"yyyyMMdd\"))\n            ).fillna(0).fillna(0.0).fillna(False)\n\n        df2 = (df\n                  .withColumn(\"whitelist\", col(\"ctn\").isin(list(set(whitelist.Number))))\n                  .withColumn(\"blocklist\", col(\"ctn\").isin(list(set(mobility_block.Number))))\n                  .withColumn(\"vm_pilot\", (col(\"ctn\").isin(list(set(vm_pilot.ctn))) | (col(\"ctn\").substr(1,3) == \"999\")))\n                  .withColumn(\"flagged\", col(\"ctn\").isin(list(set(mobility_flagged1.Number).union(set(mobility_flagged2.Number)))))\n                 )\n\n        df2.write.option(\"header\",True).partitionBy(\"Date\").option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").save(historical_robo_labels_path)\n      except Exception as e:\n        print(e)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11202095-e13a-4dc5-9739-f9bf19ed32c9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom datetime import datetime"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79456c03-3c72-4b64-bffe-5ac754ffade8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["date1 = '20220128'\ndate2 = str(datetime.now()).replace(\"-\",\"\")\nmydates = pd.date_range(date1, date2).tolist()\n\n# COMMAND ----------\n\nrun_date = str(mydates[0].date())\nroot = \"abfss://robofeed@famlisandbox.dfs.core.windows.net/robo_rpts/\"\nmobility_block_path = root +f\"research.{run_date}.MobilityBLOCK\"\nwhitelist_path = root +f\"research.{run_date}.Whitelist\"\nmobility_flagged_path1 = root +f\"research.{run_date}.RobocallMobility\"\nmobility_flagged_path2 = root +f\"research.{run_date}.RobocallMobility-8YY\"\nmobility_unblock_path = root +f\"research.{run_date}.MobilityUNBLOCK\"\nmobility_block_activity_path = root +f\"research.{run_date}.MobilityBlockActivity\"\nvm_pilot_path = 'abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/eh163e/data/robocalling/VM_ID_Pilot_Numbersd.csv'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cada963-bd21-4d09-97f3-486d460f94b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["run_date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15df184d-82b9-4985-80a9-b9350e040224"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: &#39;2022-01-29&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: &#39;2022-01-29&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["whitelist = (spark\n  .read\n  .format(\"CSV\")\n  .option(\"header\",True)\n  .option(\"delimiter\",\"|\")\n  .load(whitelist_path)\n  .toPandas())\nwhitelist_total_lines = whitelist.iloc[0][2]\nwhitelist.columns = [\"Number\",\"cnam\",\"date\",\"_c3\"]\nwhitelist = whitelist[1:]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa4fc9d3-bb59-4298-8315-eb265eaf09f0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-774409948270981&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> whitelist = (spark\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>read\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;CSV&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;header&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;delimiter&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;|&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>         self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>         <span class=\"ansi-green-fg\">elif</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> list<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o420.load.\n: Operation failed: &#34;This request is not authorized to perform this operation using this permission.&#34;, 403, HEAD, https://famlisandbox.dfs.core.windows.net/robofeed/robo_rpts/research.2022-01-28.Whitelist?upn=false&amp;action=getStatus&amp;timeout=90\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:246)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:712)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:900)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:682)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:356)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:323)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:323)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"Operation failed: &#34;This request is not authorized to perform this operation using this permission.&#34;, 403, HEAD, https://famlisandbox.dfs.core.windows.net/robofeed/robo_rpts/research.2022-01-28.Whitelist?upn=false&amp;action=getStatus&amp;timeout=90","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-774409948270981&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> whitelist = (spark\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>read\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;CSV&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;header&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;delimiter&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;|&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>         self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>         <span class=\"ansi-green-fg\">elif</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> list<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o420.load.\n: Operation failed: &#34;This request is not authorized to perform this operation using this permission.&#34;, 403, HEAD, https://famlisandbox.dfs.core.windows.net/robofeed/robo_rpts/research.2022-01-28.Whitelist?upn=false&amp;action=getStatus&amp;timeout=90\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:246)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:712)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:900)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:682)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:356)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:323)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:323)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\nhistorical_robo_labels_path = \"abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/eh163e/data/robocalling/historical_robo_features_with_labels3\"\n\nfiles = dbutils.fs.ls(\"abfss://famli-dev-workspace@famlisandbox.dfs.core.windows.net/call_graph/source/daily_refresh/\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"840ea14e-96f3-4fa5-92d3-c5632fccd4f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f19f406-dc30-472d-b866-0be2dd688444"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Continuous Score","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1283025956224074}},"nbformat":4,"nbformat_minor":0}
